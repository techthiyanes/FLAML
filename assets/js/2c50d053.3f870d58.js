"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7682],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>m});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=a.createContext({}),l=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=l(e.components);return a.createElement(p.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,p=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),d=l(n),m=r,h=d["".concat(p,".").concat(m)]||d[m]||c[m]||i;return n?a.createElement(h,o(o({ref:t},u),{},{components:n})):a.createElement(h,o({ref:t},u))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=d;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var l=2;l<i;l++)o[l]=n[l];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},3737:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=n(7462),r=(n(7294),n(3905));const i={},o=void 0,s={unversionedId:"Examples/Integrate - OpenAI",id:"Examples/Integrate - OpenAI",isDocsHomePage:!1,title:"Integrate - OpenAI",description:"FLAML offers a cost-effective hyperparameter optimization technique EcoOptiGen for tuning Large Language Models. Our study finds that tuning hyperparameters can significantly improve the utility of the OpenAI API.",source:"@site/docs/Examples/Integrate - OpenAI.md",sourceDirName:"Examples",slug:"/Examples/Integrate - OpenAI",permalink:"/FLAML/docs/Examples/Integrate - OpenAI",editUrl:"https://github.com/microsoft/FLAML/edit/main/website/docs/Examples/Integrate - OpenAI.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Integrate - AzureML",permalink:"/FLAML/docs/Examples/Integrate - AzureML"},next:{title:"Integrate - Scikit-learn Pipeline",permalink:"/FLAML/docs/Examples/Integrate - Scikit-learn Pipeline"}},p=[{value:"Prerequisites",id:"prerequisites",children:[],level:3},{value:"Load the dataset",id:"load-the-dataset",children:[],level:3},{value:"Defining the metric",id:"defining-the-metric",children:[{value:"Define a code executor",id:"define-a-code-executor",children:[],level:4},{value:"Define a function to evaluate the success for a given program synthesis task",id:"define-a-function-to-evaluate-the-success-for-a-given-program-synthesis-task",children:[],level:4}],level:3},{value:"Tuning Hyperparameters for OpenAI",id:"tuning-hyperparameters-for-openai",children:[{value:"Output tuning results",id:"output-tuning-results",children:[],level:4},{value:"Make a request with the tuned config",id:"make-a-request-with-the-tuned-config",children:[],level:4},{value:"Evaluate the success rate on the test data",id:"evaluate-the-success-rate-on-the-test-data",children:[],level:4}],level:3}],l={toc:p};function u(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},l,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"FLAML offers a cost-effective hyperparameter optimization technique ",(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2303.04673"},"EcoOptiGen")," for tuning Large Language Models. Our study finds that tuning hyperparameters can significantly improve the utility of the OpenAI API.\nIn this example, we will tune several hyperparameters for the OpenAI's completion API, including the temperature, prompt and n (number of completions), to optimize the inference performance for a code generation task."),(0,r.kt)("h3",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("p",null,"Install the ","[openai]"," option. The OpenAI integration is in preview. ChaptGPT support is available since version 1.2.0."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'pip install "flaml[openai]==1.2.0"\n')),(0,r.kt)("p",null,"Setup your OpenAI key:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nif "OPENAI_API_KEY" not in os.environ:\n    os.environ["OPENAI_API_KEY"] = "<your OpenAI API key here>"\n')),(0,r.kt)("p",null,"If you use Azure OpenAI, set up Azure using the following code:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'openai.api_type = "azure"\nopenai.api_base = "https://<your_endpoint>.openai.azure.com/"\nopenai.api_version = "2022-12-01"  # change if necessary\n')),(0,r.kt)("h3",{id:"load-the-dataset"},"Load the dataset"),(0,r.kt)("p",null,'We use the HumanEval dataset as an example. The dataset contains 164 examples. We use the first 20 for tuning the generation hyperparameters and the remaining for evaluation. In each example, the "prompt" is the prompt string for eliciting the code generation, "test" is the Python code for unit test for the example, and "entry_point" is the function name to be tested.'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import datasets\n\nseed = 41\ndata = datasets.load_dataset("openai_humaneval")["test"].shuffle(seed=seed)\nn_tune_data = 20\ntune_data = [\n    {\n        "prompt": data[x]["prompt"],\n        "test": data[x]["test"],\n        "entry_point": data[x]["entry_point"],\n    }\n    for x in range(n_tune_data)\n]\ntest_data = [\n    {\n        "prompt": data[x]["prompt"],\n        "test": data[x]["test"],\n        "entry_point": data[x]["entry_point"],\n    }\n    for x in range(n_tune_data, len(data))\n]\n')),(0,r.kt)("h3",{id:"defining-the-metric"},"Defining the metric"),(0,r.kt)("p",null,"Before starting tuning, you need to define the metric for the optimization. For the HumanEval dataset, we use the success rate as the metric. So if one of the returned responses can pass the test, we consider the task as successfully solved. Then we can define the mean success rate of a collection of tasks."),(0,r.kt)("h4",{id:"define-a-code-executor"},"Define a code executor"),(0,r.kt)("p",null,"First, we write a simple code executor. The code executor takes the generated code and the test code as the input, and execute them with a timer."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import signal\nimport subprocess\nimport sys\n\ndef timeout_handler(signum, frame):\n    raise TimeoutError("Timed out!")\n\nsignal.signal(signal.SIGALRM, timeout_handler)\nmax_exec_time = 3  # seconds\n\ndef execute_code(code):\n    code = code.strip()\n    with open("codetest.py", "w") as fout:\n        fout.write(code)\n    try:\n        signal.alarm(max_exec_time)\n        result = subprocess.run(\n            [sys.executable, "codetest.py"],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.PIPE,\n        )\n        signal.alarm(0)\n    except TimeoutError:\n        return 0\n    return int(result.returncode == 0)\n')),(0,r.kt)("p",null,'This function will create a temp file "codetest.py" and execute it in a separate process. It allows for 3 seconds to finish that code.'),(0,r.kt)("h4",{id:"define-a-function-to-evaluate-the-success-for-a-given-program-synthesis-task"},"Define a function to evaluate the success for a given program synthesis task"),(0,r.kt)("p",null,"Now we define the success metric."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def success_metrics(responses, prompt, test, entry_point):\n    """Check if the task is successful.\n\n    Args:\n        responses (list): The list of responses.\n        prompt (str): The input prompt.\n        test (str): The test code.\n        entry_point (str): The name of the function.\n\n    Returns:\n        dict: The success metrics.\n    """\n    success_list = []\n    n = len(responses)\n    for i in range(n):\n        response = responses[i]\n        code = f"{prompt}{response}\\n{test}\\ncheck({entry_point})"\n        succeed = execute_code(code)\n        success_list.append(succeed)\n    return {\n        "expected_success": 1 - pow(1 - sum(success_list) / n, n),\n        "success": any(s for s in success_list),\n    }\n')),(0,r.kt)("h3",{id:"tuning-hyperparameters-for-openai"},"Tuning Hyperparameters for OpenAI"),(0,r.kt)("p",null,"The tuning will be performed under the specified optimization budgets."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"inference_budget is the target average inference budget per instance in the benchmark. For example, 0.02 means the target inference budget is 0.02 dollars, which translates to 1000 tokens (input + output combined) if the text Davinci model is used."),(0,r.kt)("li",{parentName:"ul"},"optimization_budget is the total budget allowed to perform the tuning. For example, 5 means 5 dollars are allowed in total, which translates to 250K tokens for the text Davinci model."),(0,r.kt)("li",{parentName:"ul"},"num_sumples is the number of different hyperparameter configurations which is allowed to try. The tuning will stop after either num_samples trials or after optimization_budget dollars spent, whichever happens first. -1 means no hard restriction in the number of trials and the actual number is decided by optimization_budget.")),(0,r.kt)("p",null,"Users can specify tuning data, optimization metric, optimization mode, evaluation function, search spaces etc."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'config, analysis = oai.Completion.tune(\n    data=tune_data,  # the data for tuning\n    metric="expected_success",  # the metric to optimize\n    mode="max",  # the optimization mode\n    eval_func=success_metrics,  # the evaluation function to return the success metrics\n    # log_file_name="logs/humaneval.log",  # the log file name\n    inference_budget=0.1,  # the inference budget (dollar)\n    optimization_budget=4,  # the optimization budget (dollar)\n    # num_samples can further limit the number of trials for different hyperparameter configurations;\n    # -1 means decided by the optimization budget only\n    num_samples=-1,\n    prompt=[\n        "{prompt}",\n        "# Python 3{prompt}",\n        "Complete the following Python function:{prompt}",\n        "Complete the following Python function while including necessary import statements inside the function:{prompt}",\n    ],  # the prompt templates to choose from\n    stop=["\\nclass", "\\ndef", "\\nif", "\\nprint"],  # the stop sequence\n)\n')),(0,r.kt)("h4",{id:"output-tuning-results"},"Output tuning results"),(0,r.kt)("p",null,"After the tuning, we can print out the optimized config and the result found by FLAML:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'print("optimized config", config)\nprint("best result on tuning data", analysis.best_result)\n')),(0,r.kt)("h4",{id:"make-a-request-with-the-tuned-config"},"Make a request with the tuned config"),(0,r.kt)("p",null,"We can apply the tuned config to the request for an instance:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'responses = oai.Completion.create(context=tune_data[1], **config)\nprint(responses)\nprint(success_metrics([response["text"].rstrip() for response in responses["choices"]], **tune_data[1]))\n')),(0,r.kt)("h4",{id:"evaluate-the-success-rate-on-the-test-data"},"Evaluate the success rate on the test data"),(0,r.kt)("p",null,"You can use flaml's ",(0,r.kt)("inlineCode",{parentName:"p"},"oai.Completion.eval")," to evaluate the performance of an entire dataset with the tuned config. To do that you need to set ",(0,r.kt)("inlineCode",{parentName:"p"},"oai.Completion.data")," to the data to evaluate."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"oai.Completion.data = test_data\nresult = oai.Completion.eval(analysis.best_config, prune=False, eval_only=True)\nprint(result)\n")),(0,r.kt)("p",null,"The result will vary with the inference budget and optimization budget."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/microsoft/FLAML/blob/main/notebook/integrate_openai.ipynb"},"Link to notebook")," | ",(0,r.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_openai.ipynb"},"Open in colab")))}u.isMDXComponent=!0}}]);