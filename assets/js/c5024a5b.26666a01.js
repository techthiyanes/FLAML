"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5513],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>f});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),u=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=u(e.components);return n.createElement(s.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),p=u(a),f=r,c=p["".concat(s,".").concat(f)]||p[f]||m[f]||o;return a?n.createElement(c,i(i({ref:t},d),{},{components:a})):n.createElement(c,i({ref:t},d))}));function f(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=p;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var u=2;u<o;u++)i[u]=a[u];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},3494:(e,t,a)=>{a.r(t),a.d(t,{contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>s});var n=a(7462),r=(a(7294),a(3905));const o={},i="Zero Shot AutoML",l={unversionedId:"Use-Cases/Zero-Shot-AutoML",id:"Use-Cases/Zero-Shot-AutoML",isDocsHomePage:!1,title:"Zero Shot AutoML",description:'flaml.default is a package for zero-shot AutoML, or "no-tuning" AutoML. It uses flaml.AutoML and flaml.default.portfolio to mine good hyperparameter configurations across different datasets offline, and recommend data-dependent default configurations at runtime without expensive tuning.',source:"@site/docs/Use-Cases/Zero-Shot-AutoML.md",sourceDirName:"Use-Cases",slug:"/Use-Cases/Zero-Shot-AutoML",permalink:"/FLAML/docs/Use-Cases/Zero-Shot-AutoML",editUrl:"https://github.com/microsoft/FLAML/edit/main/website/docs/Use-Cases/Zero-Shot-AutoML.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Tune User Defined Function",permalink:"/FLAML/docs/Use-Cases/Tune-User-Defined-Function"},next:{title:"AutoML - Classification",permalink:"/FLAML/docs/Examples/AutoML-Classification"}},s=[{value:"How to Use at Runtime",id:"how-to-use-at-runtime",children:[{value:"What&#39;s the magic behind the scene?",id:"whats-the-magic-behind-the-scene",children:[],level:3},{value:"Can I check the configuration before training?",id:"can-i-check-the-configuration-before-training",children:[],level:3},{value:"Combine zero shot AutoML and hyperparameter tuning",id:"combine-zero-shot-automl-and-hyperparameter-tuning",children:[],level:3},{value:"Use your own meta-learned defaults",id:"use-your-own-meta-learned-defaults",children:[],level:3}],level:2},{value:"How to Prepare Offline",id:"how-to-prepare-offline",children:[{value:"Prepare a collection of training tasks",id:"prepare-a-collection-of-training-tasks",children:[],level:3},{value:"Prepare the candidate configurations",id:"prepare-the-candidate-configurations",children:[],level:3},{value:"Evaluate each candidate configuration on each task",id:"evaluate-each-candidate-configuration-on-each-task",children:[],level:3},{value:"Learn data-dependent defaults",id:"learn-data-dependent-defaults",children:[],level:3},{value:"&quot;Flamlize&quot; a learner",id:"flamlize-a-learner",children:[],level:3}],level:2}],u={toc:s};function d(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"zero-shot-automl"},"Zero Shot AutoML"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"flaml.default"),' is a package for zero-shot AutoML, or "no-tuning" AutoML. It uses ',(0,r.kt)("a",{parentName:"p",href:"../reference/automl/automl#automl-objects"},(0,r.kt)("inlineCode",{parentName:"a"},"flaml.AutoML"))," and ",(0,r.kt)("a",{parentName:"p",href:"../reference/default/portfolio"},(0,r.kt)("inlineCode",{parentName:"a"},"flaml.default.portfolio"))," to mine good hyperparameter configurations across different datasets offline, and recommend data-dependent default configurations at runtime without expensive tuning."),(0,r.kt)("p",null,"Zero-shot AutoML has several benefits:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"The computation cost is just training one model. No tuning is involved."),(0,r.kt)("li",{parentName:"ul"},"The decision of hyperparameter configuration is instant. No overhead to worry about."),(0,r.kt)("li",{parentName:"ul"},"Your code remains the same. No breaking of the existing workflow."),(0,r.kt)("li",{parentName:"ul"},"It requires less input from the user. No need to specify a tuning budget etc."),(0,r.kt)("li",{parentName:"ul"},"All training data are used for, guess what, training. No need to worry about holding a subset of training data for validation (and overfitting the validation data)."),(0,r.kt)("li",{parentName:"ul"},"The offline preparation can be customized for a domain and leverage the historical tuning data. No experience is wasted.")),(0,r.kt)("h2",{id:"how-to-use-at-runtime"},"How to Use at Runtime"),(0,r.kt)("p",null,'The easiest way to leverage this technique is to import a "flamlized" learner of your favorite choice and use it just as how you use the learner before. The automation is done behind the scene and you are not required to change your code. For example, if you are currently using:'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from lightgbm import LGBMRegressor\n\nestimator = LGBMRegressor()\nestimator.fit(X_train, y_train)\nestimator.predict(X_test)\n")),(0,r.kt)("p",null,"Simply replace the first line with:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from flaml.default import LGBMRegressor\n")),(0,r.kt)("p",null,"All the other code remains the same. And you are expected to get a equal or better model in most cases."),(0,r.kt)("p",null,'The current list of "flamlized" learners are:'),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"LGBMClassifier, LGBMRegressor."),(0,r.kt)("li",{parentName:"ul"},"XGBClassifier, XGBRegressor."),(0,r.kt)("li",{parentName:"ul"},"RandomForestClassifier, RandomForestRegressor."),(0,r.kt)("li",{parentName:"ul"},"ExtraTreesClassifier, ExtraTreesRegressor.")),(0,r.kt)("h3",{id:"whats-the-magic-behind-the-scene"},"What's the magic behind the scene?"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"flaml.default.LGBMRegressor")," inherits ",(0,r.kt)("inlineCode",{parentName:"p"},"lightgbm.LGBMRegressor"),", so all the APIs in ",(0,r.kt)("inlineCode",{parentName:"p"},"lightgbm.LGBMRegressor")," are still valid in ",(0,r.kt)("inlineCode",{parentName:"p"},"flaml.default.LGBMRegressor"),". The difference is, ",(0,r.kt)("inlineCode",{parentName:"p"},"flaml.default.LGBMRegressor")," decides the hyperparameter configurations based on the training data. It would use a different configuration if it is predicted to outperform the original data-independent default. If you inspect the params of the fitted estimator, you can find what configuration is used. If the original default configuration is used, then it is equivalent to the original estimator."),(0,r.kt)("p",null,"The recommendation of which configuration should be used is based on offline AutoML run results. Information about the training dataset, such as the size of the dataset will be used to recommend a data-dependent configuration. The recommendation is done instantly in negligible time. The training can be faster or slower than using the original default configuration depending on the recommended configuration. Note that there is no tuning involved. Only one model is trained."),(0,r.kt)("h3",{id:"can-i-check-the-configuration-before-training"},"Can I check the configuration before training?"),(0,r.kt)("p",null,"Yes. You can use ",(0,r.kt)("inlineCode",{parentName:"p"},"suggest_hyperparams()")," to find the suggested configuration. For example,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from flaml.default import LGBMRegressor\n\nestimator = LGBMRegressor()\nhyperparams, estimator_name, X_transformed, y_transformed = estimator.suggest_hyperparams(X_train, y_train)\nprint(hyperparams)\n")),(0,r.kt)("p",null,"If you would like more control over the training, use an equivalent, open-box way for zero-shot AutoML. For example,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from flaml.default import preprocess_and_suggest_hyperparams\n\nX, y = load_iris(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nhyperparams, estimator_class, X_transformed, y_transformed, feature_transformer, label_transformer = preprocess_and_suggest_hyperparams(\n    "classification", X_train, y_train, "lgbm"\n)\nmodel = estimator_class(**hyperparams)  # estimator_class is lightgbm.LGBMClassifier\nmodel.fit(X_transformed, y_train)  # LGBMClassifier can handle raw labels\nX_test = feature_transformer.transform(X_test)  # preprocess test data\ny_pred = model.predict(X_test)\n')),(0,r.kt)("p",null,"Note that some classifiers like XGBClassifier require the labels to be integers, while others do not. So you can decide whether to use the transformed labels ",(0,r.kt)("inlineCode",{parentName:"p"},"y_transformed")," and the label transformer ",(0,r.kt)("inlineCode",{parentName:"p"},"label_transformer"),".\nAlso, each estimator may require specific preprocessing of the data. ",(0,r.kt)("inlineCode",{parentName:"p"},"X_transformed")," is the preprocessed data, and ",(0,r.kt)("inlineCode",{parentName:"p"},"feature_transformer"),' is the preprocessor. It needs to be applied to the test data before prediction. These are automated when you use the "flamlized" learner. When you use the open-box way, pay attention to them.'),(0,r.kt)("h3",{id:"combine-zero-shot-automl-and-hyperparameter-tuning"},"Combine zero shot AutoML and hyperparameter tuning"),(0,r.kt)("p",null,"Zero Shot AutoML is fast. If tuning from the recommended data-dependent configuration is required, you can use ",(0,r.kt)("inlineCode",{parentName:"p"},"flaml.AutoML.fit()")," and set ",(0,r.kt)("inlineCode",{parentName:"p"},'starting_points="data"'),". For example,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from flaml import AutoML\nautoml = AutoML()\nautoml_settings = {\n    "task": "classification",\n    "starting_points": "data",\n    "estimator_list": ["lgbm"],\n    "time_budget": 600,\n    "max_iter": 50,\n}\nautoml.fit(X_train, y_train, **automl_settings)\n')),(0,r.kt)("p",null,"Note that if you set ",(0,r.kt)("inlineCode",{parentName:"p"},"max_iter=0")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"time_budget=None"),", you are effectively using zero-shot AutoML. When ",(0,r.kt)("inlineCode",{parentName:"p"},"estimator_list")," is omitted, the estimator together with its hyperparameter configuration will be decided in a zero-shot manner."),(0,r.kt)("h3",{id:"use-your-own-meta-learned-defaults"},"Use your own meta-learned defaults"),(0,r.kt)("p",null,"To use your own meta-learned defaults, specify the path containing the meta-learned defaults. For example,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'estimator = flaml.default.LGBMRegressor(default_location="location_for_defaults")\n')),(0,r.kt)("p",null,"Or,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'preprocess_and_suggest_hyperparams(\n    "classification", X_train, y_train, "lgbm", location="location_for_defaults"\n)\n')),(0,r.kt)("p",null,"Or,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'X_train, y_train = load_iris(return_X_y=True, as_frame=as_frame)\nautoml = AutoML()\nautoml_settings = {\n    "task": "classification",\n    "log_file_name": "test/iris.log",\n    "starting_points": "data:location_for_defaults",\n    "estimator_list": ["lgbm", "xgb_limitdepth", "rf"]\n    "max_iter": 0,\n}\nautoml.fit(X_train, y_train, **automl_settings)\n')),(0,r.kt)("p",null,"Since this is a multiclass task, it will look for the following files under ",(0,r.kt)("inlineCode",{parentName:"p"},"{location_for_defaults}/"),":"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"all/multiclass.json"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"{learner_name}/multiclass.json")," for every learner_name in the estimator_list.")),(0,r.kt)("p",null,"Read the next section to understand how to generate these files if you would like to meta-learn the defaults yourself."),(0,r.kt)("h2",{id:"how-to-prepare-offline"},"How to Prepare Offline"),(0,r.kt)("p",null,"This section is intended for:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"AutoML providers for a particular domain."),(0,r.kt)("li",{parentName:"ol"},"Data scientists or engineers who need to repeatedly train models for similar tasks with varying training data.")),(0,r.kt)("p",null,"Instead of running full hyperparameter tuning from scratch every time, one can leverage the tuning experiences in similar tasks before. While we have offered the meta-learned defaults from tuning experiences of several popular learners on benchmark datasets for classification and regression, you can customize the defaults for your own tasks/learners/metrics based on your own tuning experiences."),(0,r.kt)("h3",{id:"prepare-a-collection-of-training-tasks"},"Prepare a collection of training tasks"),(0,r.kt)("p",null,"Collect a diverse set of training tasks. For each task, extract its meta feature and save in a .csv file. For example, test/default/all/metafeatures.csv:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Dataset,NumberOfInstances,NumberOfFeatures,NumberOfClasses,PercentageOfNumericFeatures\n2dplanes,36691,10,0,1.0\nadult,43957,14,2,0.42857142857142855\nAirlines,485444,7,2,0.42857142857142855\nAlbert,382716,78,2,0.3333333333333333\nAmazon_employee_access,29492,9,2,0.0\nbng_breastTumor,104976,9,0,0.1111111111111111\nbng_pbc,900000,18,0,0.5555555555555556\ncar,1555,6,4,0.0\nconnect-4,60801,42,3,0.0\ndilbert,9000,2000,5,1.0\nDionis,374569,60,355,1.0\npoker,922509,10,0,1.0\n")),(0,r.kt)("p",null,"The first column is the dataset name, and the latter four are meta features."),(0,r.kt)("h3",{id:"prepare-the-candidate-configurations"},"Prepare the candidate configurations"),(0,r.kt)("p",null,"You can extract the best configurations for each task in your collection of training tasks by running flaml on each of them with a long enough budget. Save the best configuration in a .json file under ",(0,r.kt)("inlineCode",{parentName:"p"},"{location_for_defaults}/{learner_name}/{task_name}.json"),". For example,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'X_train, y_train = load_iris(return_X_y=True, as_frame=as_frame)\nautoml.fit(X_train, y_train, estimator_list=["lgbm"], **settings)\nautoml.save_best_config("test/default/lgbm/iris.json")\n')),(0,r.kt)("h3",{id:"evaluate-each-candidate-configuration-on-each-task"},"Evaluate each candidate configuration on each task"),(0,r.kt)("p",null,"Save the evaluation results in a .csv file. For example, save the evaluation results for lgbm under ",(0,r.kt)("inlineCode",{parentName:"p"},"test/default/lgbm/results.csv"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"task,fold,type,result,params\n2dplanes,0,regression,0.946366,{'_modeljson': 'lgbm/2dplanes.json'}\n2dplanes,0,regression,0.907774,{'_modeljson': 'lgbm/adult.json'}\n2dplanes,0,regression,0.901643,{'_modeljson': 'lgbm/Airlines.json'}\n2dplanes,0,regression,0.915098,{'_modeljson': 'lgbm/Albert.json'}\n2dplanes,0,regression,0.302328,{'_modeljson': 'lgbm/Amazon_employee_access.json'}\n2dplanes,0,regression,0.94523,{'_modeljson': 'lgbm/bng_breastTumor.json'}\n2dplanes,0,regression,0.945698,{'_modeljson': 'lgbm/bng_pbc.json'}\n2dplanes,0,regression,0.946194,{'_modeljson': 'lgbm/car.json'}\n2dplanes,0,regression,0.945549,{'_modeljson': 'lgbm/connect-4.json'}\n2dplanes,0,regression,0.946232,{'_modeljson': 'lgbm/default.json'}\n2dplanes,0,regression,0.945594,{'_modeljson': 'lgbm/dilbert.json'}\n2dplanes,0,regression,0.836996,{'_modeljson': 'lgbm/Dionis.json'}\n2dplanes,0,regression,0.917152,{'_modeljson': 'lgbm/poker.json'}\nadult,0,binary,0.927203,{'_modeljson': 'lgbm/2dplanes.json'}\nadult,0,binary,0.932072,{'_modeljson': 'lgbm/adult.json'}\nadult,0,binary,0.926563,{'_modeljson': 'lgbm/Airlines.json'}\nadult,0,binary,0.928604,{'_modeljson': 'lgbm/Albert.json'}\nadult,0,binary,0.911171,{'_modeljson': 'lgbm/Amazon_employee_access.json'}\nadult,0,binary,0.930645,{'_modeljson': 'lgbm/bng_breastTumor.json'}\nadult,0,binary,0.928603,{'_modeljson': 'lgbm/bng_pbc.json'}\nadult,0,binary,0.915825,{'_modeljson': 'lgbm/car.json'}\nadult,0,binary,0.919499,{'_modeljson': 'lgbm/connect-4.json'}\nadult,0,binary,0.930109,{'_modeljson': 'lgbm/default.json'}\nadult,0,binary,0.932453,{'_modeljson': 'lgbm/dilbert.json'}\nadult,0,binary,0.921959,{'_modeljson': 'lgbm/Dionis.json'}\nadult,0,binary,0.910763,{'_modeljson': 'lgbm/poker.json'}\n...\n")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"type")," column indicates the type of the task, such as regression, binary or multiclass.\nThe ",(0,r.kt)("inlineCode",{parentName:"p"},"result")," column stores the evaluation result, assumed the large the better. The ",(0,r.kt)("inlineCode",{parentName:"p"},"params")," column indicates which json config is used. For example 'lgbm/2dplanes.json' indicates that the best lgbm configuration extracted from 2dplanes is used.\nDifferent types of tasks can appear in the same file, as long as any json config file can be used in all the tasks. For example, 'lgbm/2dplanes.json' is extracted from a regression task, and it can be applied to binary and multiclass tasks as well."),(0,r.kt)("h3",{id:"learn-data-dependent-defaults"},"Learn data-dependent defaults"),(0,r.kt)("p",null,"To recap, the inputs required for meta-learning are:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Metafeatures: e.g., ",(0,r.kt)("inlineCode",{parentName:"li"},"{location}/all/metafeatures.csv"),"."),(0,r.kt)("li",{parentName:"ol"},"Configurations: ",(0,r.kt)("inlineCode",{parentName:"li"},"{location}/{learner_name}/{task_name}.json"),"."),(0,r.kt)("li",{parentName:"ol"},"Evaluation results: ",(0,r.kt)("inlineCode",{parentName:"li"},"{location}/{learner_name}/results.csv"),".")),(0,r.kt)("p",null,'For example, if the input location is "test/default", learners are lgbm, xgb_limitdepth and rf, the following command learns data-dependent defaults for binary classification tasks.'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"python portfolio.py --output test/default --input test/default --metafeatures test/default/all/metafeatures.csv --task binary --estimator lgbm xgb_limitdepth rf\n")),(0,r.kt)("p",null,"In a few seconds, it will produce the following files as output:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"test/default/lgbm/binary.json: the learned defaults for lgbm."),(0,r.kt)("li",{parentName:"ul"},"test/default/xgb_limitdepth/binary.json: the learned defaults for xgb_limitdepth."),(0,r.kt)("li",{parentName:"ul"},"test/default/rf/binary.json: the learned defaults for rf."),(0,r.kt)("li",{parentName:"ul"},"test/default/all/binary.json: the learned defaults for lgbm, xgb_limitdepth and rf together.")),(0,r.kt)("p",null,'Change "binary" into "multiclass" or "regression", or your own types in your "results.csv" for the other types of tasks. To update the learned defaults when more experiences are available, simply update your input files and rerun the learning command.'),(0,r.kt)("h3",{id:"flamlize-a-learner"},'"Flamlize" a learner'),(0,r.kt)("p",null,"You have now effectively built your own zero-shot AutoML solution. Congratulations!"),(0,r.kt)("p",null,'Optionally, you can "flamlize" a learner using ',(0,r.kt)("a",{parentName:"p",href:"../reference/default/estimator#flamlize_estimator"},(0,r.kt)("inlineCode",{parentName:"a"},"flaml.default.flamlize_estimator"))," for easy dissemination. For example,"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import sklearn.ensemble as ensemble\nfrom flaml.default import flamlize_estimator\n\nExtraTreesClassifier = flamlize_estimator(\n    ensemble.ExtraTreesClassifier, "extra_tree", "classification"\n)\n')),(0,r.kt)("p",null,'Then, you can share this "flamlized" ',(0,r.kt)("inlineCode",{parentName:"p"},"ExtraTreesClassifier")," together with the location of your learned defaults with others (or the ",(0,r.kt)("em",{parentName:"p"},"future")," yourself). They will benefit from your past experience. Your group can also share experiences in a central place and update the learned defaults continuously. Over time, your organization gets better collectively."))}d.isMDXComponent=!0}}]);