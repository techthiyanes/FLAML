"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5251],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>d});var a=n(7294);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,l=function(e,t){if(null==e)return{};var n,a,l={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var p=a.createContext({}),s=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,l=e.mdxType,r=e.originalType,p=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),u=s(n),d=l,k=u["".concat(p,".").concat(d)]||u[d]||m[d]||r;return n?a.createElement(k,i(i({ref:t},c),{},{components:n})):a.createElement(k,i({ref:t},c))}));function d(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=n.length,i=new Array(r);i[0]=u;var o={};for(var p in t)hasOwnProperty.call(t,p)&&(o[p]=t[p]);o.originalType=e,o.mdxType="string"==typeof e?e:l,i[1]=o;for(var s=2;s<r;s++)i[s]=n[s];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},1535:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>i,default:()=>c,frontMatter:()=>r,metadata:()=>o,toc:()=>p});var a=n(7462),l=(n(7294),n(3905));const r={sidebar_label:"completion",title:"integrations.oai.completion"},i=void 0,o={unversionedId:"reference/integrations/oai/completion",id:"reference/integrations/oai/completion",isDocsHomePage:!1,title:"integrations.oai.completion",description:"get\\_key",source:"@site/docs/reference/integrations/oai/completion.md",sourceDirName:"reference/integrations/oai",slug:"/reference/integrations/oai/completion",permalink:"/FLAML/docs/reference/integrations/oai/completion",editUrl:"https://github.com/microsoft/FLAML/edit/main/website/docs/reference/integrations/oai/completion.md",tags:[],version:"current",frontMatter:{sidebar_label:"completion",title:"integrations.oai.completion"},sidebar:"referenceSideBar",previous:{title:"suggest",permalink:"/FLAML/docs/reference/default/suggest"},next:{title:"autovw",permalink:"/FLAML/docs/reference/onlineml/autovw"}},p=[{value:"get_key",id:"get_key",children:[],level:4},{value:"Completion Objects",id:"completion-objects",children:[{value:"set_cache",id:"set_cache",children:[],level:4},{value:"eval",id:"eval",children:[],level:4},{value:"tune",id:"tune",children:[],level:4},{value:"create",id:"create",children:[],level:4}],level:2},{value:"ChatCompletion Objects",id:"chatcompletion-objects",children:[],level:2}],s={toc:p};function c(e){let{components:t,...n}=e;return(0,l.kt)("wrapper",(0,a.Z)({},s,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h4",{id:"get_key"},"get","_","key"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def get_key(config)\n")),(0,l.kt)("p",null,"Get a unique identifier of a configuration."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"config")," ",(0,l.kt)("em",{parentName:"li"},"dict or list")," - A configuration.")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Returns"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"tuple")," - A unique identifier which can be used as a key for a dict.")),(0,l.kt)("h2",{id:"completion-objects"},"Completion Objects"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"class Completion()\n")),(0,l.kt)("p",null,"A class for OpenAI completion API."),(0,l.kt)("p",null,"It also supports: ChatCompletion, Azure OpenAI API."),(0,l.kt)("h4",{id:"set_cache"},"set","_","cache"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'@classmethod\ndef set_cache(cls, seed=41, cache_path=".cache")\n')),(0,l.kt)("p",null,"Set cache path."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"seed")," ",(0,l.kt)("em",{parentName:"li"},"int, Optional")," - The integer identifier for the pseudo seed.\nResults corresponding to different seeds will be cached in different places."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"cache_path")," ",(0,l.kt)("em",{parentName:"li"},"str, Optional")," - The root path for the cache.\nThe complete cache path will be {cache_path}/{seed}.")),(0,l.kt)("h4",{id:"eval"},"eval"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"@classmethod\ndef eval(cls, config: dict, prune=True, eval_only=False)\n")),(0,l.kt)("p",null,"Evaluate the given config as the hyperparameter setting for the openai api call."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"config")," ",(0,l.kt)("em",{parentName:"li"},"dict")," - Hyperparameter setting for the openai api call."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"prune")," ",(0,l.kt)("em",{parentName:"li"},"bool, optional")," - Whether to enable pruning. Defaults to True."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"eval_only")," ",(0,l.kt)("em",{parentName:"li"},"bool, optional")," - Whether to evaluate only (ignore the inference budget and no timeout).\nDefaults to False.")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Returns"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"dict")," - Evaluation results.")),(0,l.kt)("h4",{id:"tune"},"tune"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"@classmethod\ndef tune(cls, data, metric, mode, eval_func, log_file_name=None, inference_budget=None, optimization_budget=None, num_samples=1, logging_level=logging.WARNING, **config, ,)\n")),(0,l.kt)("p",null,"Tune the parameters for the OpenAI API call."),(0,l.kt)("p",null,"TODO: support parallel tuning with ray or spark."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"data")," ",(0,l.kt)("em",{parentName:"p"},"list")," - The list of data points.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"metric")," ",(0,l.kt)("em",{parentName:"p"},"str")," - The metric to optimize.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"mode")," ",(0,l.kt)("em",{parentName:"p"},"str"),' - The optimization mode, "min" or "max.')),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"eval_func")," ",(0,l.kt)("em",{parentName:"p"},"Callable")," - The evaluation function for responses.\nThe function should take a list of responses and a data point as input,\nand return a dict of metrics. For example,"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def eval_func(responses, **data):\nsolution = data["solution"]\nsuccess_list = []\nn = len(responses)\nfor i in range(n):\nresponse = responses[i]\nsucceed = is_equiv_chain_of_thought(response, solution)\nsuccess_list.append(succeed)\nreturn {\n'))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},'"expected_success"')," - 1 - pow(1 - sum(success_list) / n, n),")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},'"success"')," - any(s for s in success_list),\n}"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre"},""))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"log_file_name")," ",(0,l.kt)("em",{parentName:"p"},"str, optional")," - The log file.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"inference_budget")," ",(0,l.kt)("em",{parentName:"p"},"float, optional")," - The inference budget.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"optimization_budget")," ",(0,l.kt)("em",{parentName:"p"},"float, optional")," - The optimization budget.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"num_samples")," ",(0,l.kt)("em",{parentName:"p"},"int, optional")," - The number of samples to evaluate.\n-1 means no hard restriction in the number of trials\nand the actual number is decided by optimization_budget. Defaults to 1.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"**config")," ",(0,l.kt)("em",{parentName:"p"},"dict")," - The search space to update over the default search.\nFor prompt, please provide a string/Callable or a list of strings/Callables."),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},'If prompt is provided for chat models, it will be converted to messages under role "user".'),(0,l.kt)("li",{parentName:"ul"},"Do not provide both prompt and messages for chat models, but provide either of them."),(0,l.kt)("li",{parentName:"ul"},"A string ",(0,l.kt)("inlineCode",{parentName:"li"},"prompt")," template will be used to generate a prompt for each data instance\nusing ",(0,l.kt)("inlineCode",{parentName:"li"},"prompt.format(**data)"),"."),(0,l.kt)("li",{parentName:"ul"},"A callable ",(0,l.kt)("inlineCode",{parentName:"li"},"prompt")," template will be used to generate a prompt for each data instance\nusing ",(0,l.kt)("inlineCode",{parentName:"li"},"prompt(data)"),'.\nFor stop, please provide a string, a list of strings, or a list of lists of strings.\nFor messages (chat models only), please provide a list of messages (for a single chat prefix)\nor a list of lists of messages (for multiple choices of chat prefix to choose from).\nEach message should be a dict with keys "role" and "content".')))),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Returns"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"dict")," - The optimized hyperparameter setting."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"tune.ExperimentAnalysis")," - The tuning results.")),(0,l.kt)("h4",{id:"create"},"create"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"@classmethod\ndef create(cls, context, use_cache=True, **config)\n")),(0,l.kt)("p",null,"Make a completion for a given context."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"context")," ",(0,l.kt)("em",{parentName:"li"},"dict")," - The context to instantiate the prompt.\nIt needs to contain keys that are used by the prompt template.\nE.g., ",(0,l.kt)("inlineCode",{parentName:"li"},'prompt="Complete the following sentence: {prefix}"'),"."),(0,l.kt)("li",{parentName:"ul"},"`",(0,l.kt)("inlineCode",{parentName:"li"},'context={"prefix"'),' - "Today I feel"}`.\nThe actual prompt sent to OpenAI will be:\n"Complete the following sentence: Today I feel".'),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"use_cache")," ",(0,l.kt)("em",{parentName:"li"},"bool, Optional")," - Whether to use cached responses.")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Returns"),":"),(0,l.kt)("p",null,"  Responses from OpenAI API."),(0,l.kt)("h2",{id:"chatcompletion-objects"},"ChatCompletion Objects"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"class ChatCompletion(Completion)\n")),(0,l.kt)("p",null,"A class for OpenAI API ChatCompletion."))}c.isMDXComponent=!0}}]);