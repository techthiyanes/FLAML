<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v0.0.0-4193">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous"><title data-react-helmet="true">Auto Generation | FLAML</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://microsoft.github.io//FLAML/docs/Use-Cases/Auto-Generation"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Auto Generation | FLAML"><meta data-react-helmet="true" name="description" content="flaml.autogen is a subpackage for automating generation tasks. It uses flaml.tune to find good hyperparameter configurations under budget constraints."><meta data-react-helmet="true" property="og:description" content="flaml.autogen is a subpackage for automating generation tasks. It uses flaml.tune to find good hyperparameter configurations under budget constraints."><link data-react-helmet="true" rel="shortcut icon" href="/FLAML/img/flaml_logo.ico"><link data-react-helmet="true" rel="canonical" href="https://microsoft.github.io//FLAML/docs/Use-Cases/Auto-Generation"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io//FLAML/docs/Use-Cases/Auto-Generation" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io//FLAML/docs/Use-Cases/Auto-Generation" hreflang="x-default"><link rel="stylesheet" href="/FLAML/assets/css/styles.d234052d.css">
<link rel="preload" href="/FLAML/assets/js/runtime~main.857abbd1.js" as="script">
<link rel="preload" href="/FLAML/assets/js/main.0970178a.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/FLAML/"><div class="navbar__logo"><img src="/FLAML/img/flaml_logo_fill.svg" alt="FLAML" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/FLAML/img/flaml_logo_fill.svg" alt="FLAML" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">FLAML</b></a><a class="navbar__item navbar__link navbar__link--active" href="/FLAML/docs/Getting-Started">Docs</a><a class="navbar__item navbar__link" href="/FLAML/docs/reference/automl/automl">SDK</a><a class="navbar__item navbar__link" href="/FLAML/docs/FAQ">FAQ</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/microsoft/FLAML" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ðŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ðŸŒž</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/FLAML/docs/Getting-Started">Getting Started</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/FLAML/docs/Installation">Installation</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#">Use Cases</a><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/FLAML/docs/Use-Cases/Auto-Generation">Auto Generation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/FLAML/docs/Use-Cases/Task-Oriented-AutoML">Task Oriented AutoML</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/FLAML/docs/Use-Cases/Tune-User-Defined-Function">Tune User Defined Function</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/FLAML/docs/Use-Cases/Zero-Shot-AutoML">Zero Shot AutoML</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Examples</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/FLAML/docs/Contribute">Contributing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/FLAML/docs/Research">Research</a></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Auto Generation</h1></header><p><code>flaml.autogen</code> is a subpackage for automating generation tasks. It uses <a href="/FLAML/docs/reference/tune/tune"><code>flaml.tune</code></a> to find good hyperparameter configurations under budget constraints.
Such optimization has several benefits:</p><ul><li>Maximize the utility out of using expensive foundation models.</li><li>Reduce the inference cost by using cheaper models or configurations which achieve equal or better performance.</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="choices-to-optimize">Choices to Optimize<a aria-hidden="true" class="hash-link" href="#choices-to-optimize" title="Direct link to heading">â€‹</a></h2><p>The cost of using foundation models for text generation is typically measured in terms of the number of tokens in the input and output combined. From the perspective of an application builder using foundation models, the use case is to maximize the utility of the generated text under an inference budget constraint (e.g., measured by the average dollar cost needed to solve a coding problem). This can be achieved by optimizing the hyperparameters of the inference,
which can significantly affect both the utility and the cost of the generated text.</p><p>The tunable hyperparameters include:</p><ol><li>model - this is a required input, specifying the model ID to use.</li><li>prompt/messages - the input prompt/messages to the model, which provides the context for the text generation task.</li><li>max_tokens - the maximum number of tokens (words or word pieces) to generate in the output.</li><li>temperature - a value between 0 and 1 that controls the randomness of the generated text. A higher temperature will result in more random and diverse text, while a lower temperature will result in more predictable text.</li><li>top_p - a value between 0 and 1 that controls the sampling probability mass for each token generation. A lower top_p value will make it more likely to generate text based on the most likely tokens, while a higher value will allow the model to explore a wider range of possible tokens.</li><li>n - the number of responses to generate for a given prompt. Generating multiple responses can provide more diverse and potentially more useful output, but it also increases the cost of the request.</li><li>stop - a list of strings that, when encountered in the generated text, will cause the generation to stop. This can be used to control the length or the validity of the output.</li><li>presence_penalty, frequency_penalty - values that control the relative importance of the presence and frequency of certain words or phrases in the generated text.</li><li>best_of - the number of responses to generate server-side when selecting the &quot;best&quot; (the one with the highest log probability per token) response for a given prompt.</li></ol><p>The cost and utility of text generation are intertwined with the joint effect of these hyperparameters.
There are also complex interactions among subsets of the hyperparameters. For example,
the temperature and top_p are not recommended to be altered from their default values together because they both control the randomness of the generated text, and changing both at the same time can result in conflicting effects; n and best_of are rarely tuned together because if the application can process multiple outputs, filtering on the server side causes unnecessary information loss; both n and max_tokens will affect the total number of tokens generated, which in turn will affect the cost of the request.
These interactions and trade-offs make it difficult to manually determine the optimal hyperparameter settings for a given text generation task.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="tune-hyperparameters">Tune Hyperparameters<a aria-hidden="true" class="hash-link" href="#tune-hyperparameters" title="Direct link to heading">â€‹</a></h2><p>The tuning can be performed with the following information:</p><ol><li>Validation data.</li><li>Evaluation function.</li><li>Metric to optimize.</li><li>Search space.</li><li>Budgets: inference and optimization respectively.</li></ol><h3 class="anchor anchorWithStickyNavbar_y2LR" id="validation-data">Validation data<a aria-hidden="true" class="hash-link" href="#validation-data" title="Direct link to heading">â€‹</a></h3><p>Collect a diverse set of instances. They can be stored in an iterable of dicts. For example, each instance dict can contain &quot;problem&quot; as a key and the description str of a math problem as the value; and &quot;solution&quot; as a key and the solution str as the value.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="evaluation-function">Evaluation function<a aria-hidden="true" class="hash-link" href="#evaluation-function" title="Direct link to heading">â€‹</a></h3><p>The evaluation function should take a list of responses, and other keyword arguments corresponding to the keys in each validation data instance as input, and output a dict of metrics. For example,</p><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">eval_math_responses</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">responses</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> List</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token builtin" style="color:rgb(130, 170, 255)">str</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> solution</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">str</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">**</span><span class="token plain">args</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token operator" style="color:rgb(137, 221, 255)">&gt;</span><span class="token plain"> Dict</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># select a response from the list of responses</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># check whether the answer is correct</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;success&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token boolean" style="color:rgb(255, 88, 116)">True</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">or</span><span class="token plain"> </span><span class="token boolean" style="color:rgb(255, 88, 116)">False</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p><a href="/FLAML/docs/reference/autogen/code_utils"><code>flaml.autogen.code_utils</code></a> and <a href="/FLAML/docs/reference/autogen/math_utils"><code>flaml.autogen.math_utils</code></a> offer some example evaluation functions for code generation and math problem solving.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="metric-to-optimize">Metric to optimize<a aria-hidden="true" class="hash-link" href="#metric-to-optimize" title="Direct link to heading">â€‹</a></h3><p>The metric to optimize is usually an aggregated metric over all the tuning data instances. For example, users can specify &quot;success&quot; as the metric and &quot;max&quot; as the optimization mode. By default, the aggregation function is taking the average. Users can provide a customized aggregation function if needed.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="search-space">Search space<a aria-hidden="true" class="hash-link" href="#search-space" title="Direct link to heading">â€‹</a></h3><p>Users can specify the (optional) search range for each hyperparameter.</p><ol><li>model. Either a constant str, or multiple choices specified by <code>flaml.tune.choice</code>.</li><li>prompt/messages. Prompt is either a str or a list of strs, of the prompt templates. messages is a list of dicts or a list of lists, of the message templates.
Each prompt/message template will be formatted with each data instance. For example, the prompt template can be:
&quot;{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in <!-- -->\<!-- -->boxed{{}}.&quot;
And <code>{problem}</code> will be replaced by the &quot;problem&quot; field of each data instance.</li><li>max_tokens, n, best_of. They can be constants, or specified by <code>flaml.tune.randint</code>, <code>flaml.tune.qrandint</code>, <code>flaml.tune.lograndint</code> or <code>flaml.qlograndint</code>. By default, max_tokens is searched in [50, 1000); n is searched in [1, 100); and best_of is fixed to 1.</li><li>stop. It can be a str or a list of strs, or a list of lists of strs or None. Default is None.</li><li>temperature or top_p. One of them can be specified as a constant or by <code>flaml.tune.uniform</code> or <code>flaml.tune.loguniform</code> etc.
Please don&#x27;t provide both. By default, each configuration will choose either a temperature or a top_p in <!-- -->[0, 1]<!-- --> uniformly.</li><li>presence_penalty, frequency_penalty. They can be constants or specified by <code>flaml.tune.uniform</code> etc. Not tuned by default.</li></ol><h3 class="anchor anchorWithStickyNavbar_y2LR" id="budgets">Budgets<a aria-hidden="true" class="hash-link" href="#budgets" title="Direct link to heading">â€‹</a></h3><p>One can specify an inference budget and an optimization budget.
The inference budget refers to the average inference cost per data instance.
The optimization budget refers to the total budget allowed in the tuning process. Both are measured by dollars and follow the price per 1000 tokens.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="perform-tuning">Perform tuning<a aria-hidden="true" class="hash-link" href="#perform-tuning" title="Direct link to heading">â€‹</a></h3><p>Now, you can use <a href="/FLAML/docs/reference/autogen/oai/completion#tune"><code>flaml.oai.Completion.tune</code></a> for tuning. For example,</p><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> flaml </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> oai</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> analysis </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> oai</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">Completion</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">tune</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">tune_data</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    metric</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;success&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    mode</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;max&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    eval_func</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">eval_func</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    inference_budget</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">0.05</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    optimization_budget</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">3</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    num_samples</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p><code>num_samples</code> is the number of configurations to sample. -1 means unlimited (until optimization budget is exhausted).
The returned <code>config</code> contains the optimized configuration and <code>analysis</code> contains an <a href="/FLAML/docs/reference/tune/analysis#experimentanalysis-objects">ExperimentAnalysis</a> object for all the tried configurations and results.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="perform-inference-with-the-tuned-config">Perform inference with the tuned config<a aria-hidden="true" class="hash-link" href="#perform-inference-with-the-tuned-config" title="Direct link to heading">â€‹</a></h2><p>One can use <a href="/FLAML/docs/reference/autogen/oai/completion#create"><code>flaml.oai.Completion.create</code></a> to performance inference.
There are a number of benefits of using <code>flaml.oai.Completion.create</code> to perform inference.</p><p>A template is either a format str, or a function which produces a str from several input fields.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="api-unification">API unification<a aria-hidden="true" class="hash-link" href="#api-unification" title="Direct link to heading">â€‹</a></h3><p><code>flaml.oai.Completion.create</code> is compatible with both <code>openai.Completion.create</code> and <code>openai.ChatCompletion.create</code>, and both OpenAI API and Azure OpenAI API. So models such as &quot;text-davinci-003&quot;, &quot;gpt-3.5-turbo&quot; and &quot;gpt-4&quot; can share a common API. When only tuning the chat-based models, <code>flaml.oai.ChatCompletion</code> can be used.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="caching">Caching<a aria-hidden="true" class="hash-link" href="#caching" title="Direct link to heading">â€‹</a></h3><p>API call results are cached locally and reused when the same request is issued. This is useful when repeating or continuing experiments for reproducibility and cost saving. It still allows controlled randomness by setting the &quot;seed&quot;, using <a href="/FLAML/docs/reference/autogen/oai/completion#set_cache"><code>set_cache</code></a> or specifying in <code>create()</code>.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="error-handling">Error handling<a aria-hidden="true" class="hash-link" href="#error-handling" title="Direct link to heading">â€‹</a></h3><p>It is easy to hit error when calling OpenAI APIs, due to connection, rate limit, or timeout. Some of the errors are transient. <code>flaml.oai.Completion.create</code> deals with the transient errors and retries automatically. Initial request timeout, retry timeout and retry time interval can be configured via <code>flaml.oai.request_timeout</code>, <code>flaml.oai.retry_timeout</code> and <code>flaml.oai.retry_time</code>.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="templating">Templating<a aria-hidden="true" class="hash-link" href="#templating" title="Direct link to heading">â€‹</a></h3><p>If the provided prompt or message is a template, it will be automatically materialized with a given context. For example,</p><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">response </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> oai</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">Completion</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">create</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">problme</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">problem</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> prompt</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;{problem} Solve the problem carefully.&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">**</span><span class="token plain">config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="other-utilities">Other utilities<a aria-hidden="true" class="hash-link" href="#other-utilities" title="Direct link to heading">â€‹</a></h2><p><code>flaml.oai.Completion</code> also offers some additional utilities, such as:</p><ul><li>a <a href="/FLAML/docs/reference/autogen/oai/completion#cost"><code>cost</code></a> function to calculate the cost of an API call.</li><li>a <a href="/FLAML/docs/reference/autogen/oai/completion#test"><code>test</code></a> function to conveniently evaluate the configuration over test data.</li><li>a <a href="/FLAML/docs/reference/autogen/oai/completion#extract_text"><code>extract_text</code></a> function to extract the text from a completion or chat response.</li><li>a <a href="/FLAML/docs/reference/autogen/oai/completion#extract_text"><code>set_cache</code></a> function to set the seed and cache path. The caching is introduced in the section above, with the benefit of cost saving, reproducibility, and controlled randomness.</li></ul><p>Interested in trying it yourself? Please check the following notebook examples:</p><ul><li><a href="https://github.com/microsoft/FLAML/blob/main/notebook/autogen_openai.ipynb" target="_blank" rel="noopener noreferrer">Optimize for Code Gen</a></li><li><a href="https://github.com/microsoft/FLAML/blob/main/notebook/autogen_chatgpt.ipynb" target="_blank" rel="noopener noreferrer">Optimize for Math</a></li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/microsoft/FLAML/edit/main/website/docs/Use-Cases/Auto-Generation.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_mt2f"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/FLAML/docs/Installation"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« <!-- -->Installation</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/FLAML/docs/Use-Cases/Task-Oriented-AutoML"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Task Oriented AutoML<!-- --> Â»</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#choices-to-optimize" class="table-of-contents__link toc-highlight">Choices to Optimize</a></li><li><a href="#tune-hyperparameters" class="table-of-contents__link toc-highlight">Tune Hyperparameters</a><ul><li><a href="#validation-data" class="table-of-contents__link toc-highlight">Validation data</a></li><li><a href="#evaluation-function" class="table-of-contents__link toc-highlight">Evaluation function</a></li><li><a href="#metric-to-optimize" class="table-of-contents__link toc-highlight">Metric to optimize</a></li><li><a href="#search-space" class="table-of-contents__link toc-highlight">Search space</a></li><li><a href="#budgets" class="table-of-contents__link toc-highlight">Budgets</a></li><li><a href="#perform-tuning" class="table-of-contents__link toc-highlight">Perform tuning</a></li></ul></li><li><a href="#perform-inference-with-the-tuned-config" class="table-of-contents__link toc-highlight">Perform inference with the tuned config</a><ul><li><a href="#api-unification" class="table-of-contents__link toc-highlight">API unification</a></li><li><a href="#caching" class="table-of-contents__link toc-highlight">Caching</a></li><li><a href="#error-handling" class="table-of-contents__link toc-highlight">Error handling</a></li><li><a href="#templating" class="table-of-contents__link toc-highlight">Templating</a></li></ul></li><li><a href="#other-utilities" class="table-of-contents__link toc-highlight">Other utilities</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023 FLAML Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/FLAML/assets/js/runtime~main.857abbd1.js"></script>
<script src="/FLAML/assets/js/main.0970178a.js"></script>
</body>
</html>